---
title: "Bagging & Boosting"
author: "Andrea Spano'"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
      font_adjustment: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Random Forest

## Introduction

* Random Forest is often considered to be a panacea of all data science problems. On a funny note, when you can’t think of any algorithm (irrespective of situation), use random forest!

* Random Forest is a versatile machine learning method capable of performing both _regression_ and _classification_ tasks. 

* Five Basic concepts:
  1. Ensemble
  1. Bagging
  1. Sampling
  1. OOB
  1. Importance

## Ensembling

* When building a tree model, the algorithm works by making the best possible choice at each particular stage, without any consideration of whether those choices remain optimal in future stages. 

* That is, the algorithm makes a locally optimal decision at each stage 

* It is thus quite possible that such a choice at one stage turns out to be sub-optimal in the overall scheme

* We can try to learn from multiple trees. Just be sure they do not just learn all the same

## Bagging 

* __Bagging__: __Bootstrap aggregating__ is a method to learn from multiple trees 

* Take $m$ samples with replacement from original data of dimension  $n \times k$ so that each sample is of dimensions $n \times k$ 

* Fit a fully grown tree model (not pruned) on each sample. Minimum size of terminal nodes is usually one for classification trees and five for regression trees.
  
* When predicting, we calculate the predictions on a the test set along all trees

* Combine the predictions results into a single prediction by 
   * average for regression trees
   * majority vote for classification tree
    
## Bagging 

* Pros 
  * Combing multiple results reduce the prediction variances
  * Easy to paralelize
  
* Cons
  * Higly correlated trees. 
  * Bagging builds trees based on the same set of predictors, few strong predictors will be repeatedly selected. 
  * This leads to generating similar trees that produce highly correlated predictions. 
  * One problem with correlated predictions is that taking the average of those does not decrease variance as expected.

<!------------------------  ----------------------->

## Bagging 

* An average of $N$ i.i.d random variables, each with variance $\sigma^2$, has variance: $\frac{\sigma^2}{N}$

* In case of $N$ i.d., identical but not independent, with $\rho$ pair correlation, then the variance is:

$$ \rho\sigma^2 + \frac{1-\rho}{N}\sigma^2$$


* As $N$ increases the second term disappears but the first term remains

<!------------------------  ----------------------->

## Sampling

* As in bagging, we build a number of decision trees on bootstrapped training samples (bagging) 

* Each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors. 

* In Random forest: $m<<p$ usually:
   * $m = \frac{1}{2}\sqrt{p}$
   * $m = \sqrt{p}$
   * $m = 2\sqrt{p}$
   

* Note that if $m = p$, then this is bagging.
<!------------------------  ----------------------->

## OOB

* Remember, in bootstrapping we sample with replacement, and therefore not all observations are used for each bootstrap sample. On average $\frac{1}{3}$ of them are not used 

* We call them out-of-bag samples (OOB)

* We can predict the response for the $i_{th}$ observation using each of the trees in which that observation was OOB and do this for $n$ observations

* Calculate overall $OOB$ $MSE$ or classification error

<!------------------------  ----------------------->

## Variable Importance Measure

* Bagging results in improved accuracy over prediction using a single tree 

* Unfortunately, the resulting model becomes difficult to interpret. 

* Bagging improves prediction accuracy at the expense of interpretability. 

* A proxy for model interpretation consists of calculating the total amount that the RSS or Gini index is decreased due to splits over a given predictor and averaged over all B trees. 

* We do this for each tree over all its corresponding OOB samples to get the mean and SD

* This importance score gives an indication of how useful the variables are for prediction

<!------------------------  ----------------------->

## Random Forests Tuning

* The inventors of random Forest make the following recommendations: 

    * For classification, the default value for $m$ is  $\sqrt(p)$ and the minimum node size is $one$. 
    * For regression, the default value for $m$ is $\frac{p}{3}$ and the minimum node size is $five$. 

* In practice the best values for these parameters will depend on the problem, and __they should be treated as tuning parameters__. 

* We can use OOB data to perform cross-validation along the way. 

* Once the OOB error stabilizes, the training can be terminated: _caret_

<!------------------------  ----------------------->

## Overfitting

* Random forests _cannot overfit_ data with regards to to the number of trees.

* The number of trees does not mean increase in the flexibility of the model 


<!------------------------  ----------------------->

## randomForest

RandomForest in R is implemented via function `randomForest()` of the homonymous package




* `ntree (N)`: Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.


* `mtry	(m)` Number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)


* `weights` A vector of length same as y that are positive weights used only in sampling data to grow each tree (not used in any other calculation)

* `replace`:	Should sampling of cases be done with or without replacement?

`classwt`: Priors of the classes. Need not add up to one. Ignored for regression.

* `cutoff:` (Classification only) A vector of length equal to number of classes. The ‘winning’ class for an observation is the one with the maximum ratio of proportion of votes to cutoff. Default is 1/k where k is the number of classes (i.e., majority vote wins).

* `strata`: A (factor) variable that is used for stratified sampling.

* `sampsize`: Size(s) of sample to draw. For classification, if sampsize is a vector of the length the number of strata, then sampling is stratified by strata, and the elements of sampsize indicate the numbers to be drawn from the strata.

* `nodesize`: Minimum size of terminal nodes. Setting this number larger causes smaller trees to be grown (and thus take less time). Note that the default values are different for classification (1) and regression (5).

* `maxnodes`:	Maximum number of terminal nodes trees in the forest can have. If not given, trees are grown to the maximum possible (subject to limits by nodesize). If set larger than maximum possible, a warning is issued.

* `importance`: Should importance of predictors be assessed?

* `localImp`: Should casewise importance measure be computed? (Setting this to TRUE will override importance.)

* `nPerm`: Number of times the OOB data are permuted per tree for assessing variable importance. Number larger than 1 gives slightly more stable estimate, but not very effective. Currently only implemented for regression.

`proximity`: Should proximity measure among the rows be calculated?

`oob.prox	`: Should proximity be calculated only on “out-of-bag” data?

`norm.votes`: If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs). Ignored for regression.

