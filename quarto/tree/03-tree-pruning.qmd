## Pruning 
### Bias-Variance Tradeoff

-   Bias-variance tradeoff in machine learning is a tradeoff between:

    -   the degree to which a model fits the training data
    -   its predictive accuracy

-   This refers to the general rule that beyond a point:
    - it is counterproductive to improve the fit of a model to the training data
    - as this increases the likelihood of overfitting

-   It is easy to see that deep trees are more likely to overfit the data than shallow ones.

## Pruning
### Bias-Variance Tradeoff

-   One obvious way to control such overfitting is to construct shallower trees by stopping the algorithm at an appropriate point based on whether a split significantly improves the fit.

-   Another is to grow a tree unrestricted and then prune it back using an appropriate criterion.

-   We take the latter approach.

## Pruning
### The idea

- Starting from a (almost) fully grown tree we can start removing nodes in along a bottom-up path. 

- At each prune we have a parent node and two child nodes

- We define the __Cost__ of the parent node as:
$$C_P = \frac{n_P}{N} G_P + \alpha$$

- We define the __Cost__ of the two child nodes as:

$$C_C = \frac{n_L}{N} G_L + \frac{n_R}{N} G_R + 2\alpha$$

- We decide to prune child node if:

$$C_C \le C_P$$



## Pruning
### The idea

- Clearly $\alpha$ plays a crucial role
- For any split a specific $\tilde{\alpha}$ exists so that:

$$C_C = C_P$$

- Infact:

$$\frac{n_L}{N} G_L + \frac{n_R}{N} G_R + 2\alpha = \frac{n_P}{N} G_P + \alpha$$

- Leads to:

$$\tilde{\alpha} = \frac{n G_P - n_L G_L - n_R G_R}{N}$$

- Therefore the sequence of $\tilde{\alpha}$ is given (pre computed) when the tree is fully growt  

## Pruning
### Example


::: columns
::: column

![](fig/simple-tree-02.png)
:::

::: column
$$\tilde{\alpha} = \frac{n G_P - n_L G_L - n_R G_R}{N}$$

$$ \tilde{\alpha}_{red} = \frac{6 \cdot 0.278 - 3 \cdot 0.444 - 3 \cdot 0 }{10}$$

$$ \tilde{\alpha}_{red} = 0.0336 $$
$$ \tilde{\alpha}_{blue} = 0.333 $$

$$ \tilde{\alpha}_{green} = 0.5 $$


:::
:::


##  Pruning
### Example

![](fig/4-plot.png)

## Pruning
### Choosing $\alpha$

- In conclusion, the growth phase maps the data onto a split support ( e spieagare meglio)  

- The pruning phase works bottom-up along a predetermined series of $\alpha$

- The key point is about finding the __OPTIMAL__ $\alpha$

- Where OPTIMAL means: find that specific $\alpha$ that maximise  __Prediction Performance__ measured on a validation set


##  Pruning
### Example

- Suppose validation set being:

$$y = [1,1,1,0,0,0]$$ $$x_1 = [a,a,b,b,b,b]$$ $$x_2 = [k,h,k,h,k,h]$$

- Then:

    - When $\alpha = 0$ then $Acc = \frac{5}{6}$
    - When $\alpha = 0.0333$ then $Acc = \frac{5}{6}$
    - When $\alpha = 0.3333$ then $Acc = \frac{1}{2}$

- Therefore we chose $\alpha = 0.0333$








