


## Growth

-   In the whole data set we have a response variable $y$ and a set of predictors: $X = [x_1, \dots x_p]$

-   In order to grow the tree we need to recursively split the whole dataset into smaller datasets by using the information power contained in the relationship between $y$ and $X$.

-   Each split is made on a single variable $x_i$ at the time

-   At each split we chose the $x_i$ that **maximizes** the **Gini Gain** at that split

-   Gini Gain is not the only available KPI but for the moment we stick on it

```{=html}
<!-- 
* To split the nodes, the minimum _within-node variability_, is searched.

* For CT Variability is usually measured with two alternative indices:

    - Gini Index.
    - Highest Information Gain 

* For RT variability is measured by 

    - Total Residual Sum of Squares  (RSS)

This is an HTML-style comment 
Assume a class made of: $4A$ , $3B$ and  $3C$ for a total of $10$ observations, the probability (frequency) of each class:

* $P(A) =  0.4$, $P(B) = 0.3$ and $P(C) = 0.3$
-->
```

::: {style="text-align:center; margin-top:0%"}
# Growth
:::

## Growth - Gini Index

-   Gini index is computed as: $$G = 1 - \sum_{i=1}^{K}p{_i}{^2} $$
-   where
    -   K = the number classes in the node
    -   $p_i$ is the proportion of class $i$ in the node.\
-   $G=0$ for a *pure* class and max(G) = 1
-   $G$ is always between 0 and 1 regardless the size of K
-   Gini index is a numerical way to measure node purity: the lower the Gini, the higher the purity of the node.

## Growth - Example: Compute Gini Index

-   Assuming that in a node we have a respnse variable $y$ with tow classes $[0,1]$ with $p_1=0.5$ and therefore $p_2 = 0.5$ i.e.:

$$y = [1,1,1,1,1,0,0,0,0,0]$$

-   The value of $G$ will be:

$$G = 1 - 0.5^2 - 0.5^2 = 0.5$$

-   And infact node is pure at 50%

## Growth - Split Gini Index

-   We split a node into two child nodes using an explatory variable i.e.:

    -   $x_1$ with categories $[a,b]$
    -   $n_a$ number of categories $a$ in $x1$
    -   $n_b$ number of categories $b$ in $x1$
    -   $n = n_a + n_b$ number of observations in parent node

-   We can compute Gini index within each child node: $G(a)$ and $G(b)$.

-   $G_{x_1}$: the weighted average of $G(a)$ and $G(b)$ with $n_a$ and $n_b$, represents the Gini index after the split on $x_1$.

$$G_{x_1} = \frac{n_a}{n} G(a) + \frac{n_b}{n} G(b)$$

## Growth - Gini Gain

-   When we split a node on $x_1$ into two child nodes, the Gini Gain is: $$GG = G - G_{x_1}$$

-   Gini Gain says: “how much cleaner do the groups get after splitting?”

    -   Cleaner or same: $GG ≥ 0$

    -   Dirtier: $GG < 0$ --- In practice algorithm skips it

## Growth - Example: Split the parent node

-   Suppose now we we have two independent variables along with $y$

$$y = [1,1,1,1,1,0,0,0,0,0]$$ $$x_1 = [a, a, a, a, b, b,b,b,b,b]$$ $$x_2 = [k,h,k,h,k, h,k,h,k,h]$$

-   we can try to split the parent node into two child nodes using $x_1$ and $x_2$

## Growth - Split on $x_1$

Given that: $$y = [1,1,1,1,1,0,0,0,0,0]$$ $$x_1 = [a, a, a, a, b, b,b,b,b,b]$$

Splitting on $x_1$ means:

$$y_a = [1,1,1,1] \quad y_b = [1,0,0,0,0,0]$$

Therefore:

$$p_{{1(a)}} = \frac{4}{4} = 1.0 \quad p_{{0(a)}} = \frac{0}{4} = 0.0$$ $$p_{{1(b)}} = \frac{1}{6} = 0.17 \quad p_{{0(b)}} = \frac{5}{6} = 0.83$$

## Growth - Split on $x1$

Gini index in nodes $a$ & $b$ is:

$$G_a = 1 - \sum_{i=1}^{K}p{_i}{^2} =  1 - (1^2 + 0^2) = 0$$


$$G_b = 1 - \sum_{i=1}^{K}p{_i}{^2} = 1 - (0.17^2 + 0.83^2)  = 0.28$$

Gini index of the split is:

$$G_{x_1} = \frac{n_a}{n} G(a) + \frac{n_b}{n} G(b) = \frac{4}{10}(0) + \frac{6}{10}(0.28) = 0.17$$

Gini gain after the split is:

$$GG_{x_1} = G - G_{x_1} =  0.5 - 0.17 = 0.33$$

## Growth - Split on $x2$

Repeat calculation

## Growth - Chose your next split variable

Given the Gini Gains ($GG$) for $x_1$ and $x_2$ are:

$$GG_{x_1} =  0.33$$ $$GG_{x_2} =  0.02$$

-   We decide to split on $x_1$

## Growth - Stop Rule

-   Recursively splitting we can grow very large, but not infinitive, trees
-   We can control tree growt by using these parameters:

::: {.small-table style="font-size:50%"}
| Parameter | Description | Effect on Growth |
|-------------------|-----------------------|-------------------------------|
| `max_depth` | Maximum number of levels in the tree. | Directly limits height → smaller tree. |
| `min_samples_split` | Minimum number of samples needed to split a node. | Higher → fewer splits. |
| `min_samples_leaf` | Minimum samples required in a leaf node. | Higher → fewer, larger leaves. |
| `max_leaf_nodes` | Maximum number of leaf nodes. | Caps complexity regardless of depth. |
| **`min_impurity_decrease`** | Minimum impurity decrease required for a split. | Higher → only significant splits allowed. |
| `min_weight_fraction_leaf` | Like `min_samples_leaf`, but as a fraction of total sample weight. | Useful with weighted samples. |
:::

## Growth - Impurity Decrease

-   The Impurity Decrease is:

$$ ID = \frac{N_p}{N} GG$$

-   Where

-   $N_p = n_a + n_b$: Number of observations in parent node

-   $N$: number of observation in the whole data set

-   $GG$: Gini Gain when splitting parent node

-   The fraction $\frac{N_p}{N}$ scales the Gini gain by how big this node is compared to the whole dataset.

-   This prevents very small nodes from causing splits unless the improvement is substantial.

## Stop Rules

::: {style="font-size:50%"}
1.  Check if the node is **pure** (all samples same class):
    -   Yes → make it a leaf and stop.\
    -   No → continue.
2.  Check if the node has reached **`max_depth`**:
    -   Yes → make it a leaf and stop.\
    -   No → continue.
3.  Check if **number of samples** in the node is **less than `min_samples_split`**:
    -   Yes → make it a leaf and stop.\
    -   No → continue.
4.  Search for the **best split** among allowed features (respecting `max_features`).
5.  Check if a **valid split** is found where both children satisfy:
    -   `min_samples_leaf`\
    -   `min_weight_fraction_leaf`\
    -   If not → make it a leaf and stop.
6.  Check if **impurity decrease ≥ `min_impurity_decrease`**:
    -   No → make it a leaf and stop.\
    -   Yes → continue.
7.  Check if **splitting would exceed `max_leaf_nodes`**:
    -   Yes → make it a leaf and stop.\
    -   No → commit the split.
8.  **Repeat** the same checks for the left and right child nodes (recursion).
:::

## Growth - Summary

-   At the end of the growth process we have a very large tree

-   This large tree in almost usless

-   It is just a very **over-fitted** model

-   To avoid over-fitting we need pruning the tree up to an optimumum level

-   Pruning is about reducing tree side walking bottom-up


## Example 


```{python}
#| echo: true

from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
import pandas as pd



df = pd.DataFrame({
    "y" : [1,1,1,1,1,0,0,0,0,0], 
    "x1" : [0,0,0,0,1,1,1,1,1,1],
    "x2" : [0,1,0,1,0,1,0,1,0,1]
 })


# Response 
y = df["y"]

# Predictors 
X = df.drop(columns=["y"])

```
## Example 

```{python}
#| echo: true
# Model

model = DecisionTreeClassifier(
   criterion="gini",
   min_samples_split = 2, 
   min_samples_leaf = 1,   
   min_impurity_decrease = 0,  
   max_depth=None,
   random_state=46);




# Fit
model.fit(X, y)

```



## Example

::: columns
::: column

```{python}
#| echo: true
# Tree Plot
fig, ax = plt.subplots(figsize=(7,5))
plot_tree(
    model,
    filled=False,        
    feature_names=['x1','x2'], 
    class_names=None,  
    impurity=True,       
    proportion=False,    
    label="all",        
    fontsize=10         
)
plt.close(fig) 
```

:::

::: column
```{python}
display (fig)
```
:::
:::


